#trained the final RoBERTa model on the full dataset in Colab, because Server crashed.
# -*- coding: utf-8 -*-
"""Roberta_Training_Final_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnaZlbUuj62C39Mug5NKt0oZRdARRQGt
"""

from google.colab import drive
drive.mount('/content/drive')

#!pip install transformers
#!pip install datasets
#!pip install evaluate

#Roberta Training of Final Model
from pandas import json_normalize
from datasets import load_dataset, DatasetDict
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from sklearn.metrics import f1_score
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import ipykernel
import evaluate
import torch
import os
import random

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#print(f"Using device: {device}")

from transformers import (
    DataCollatorWithPadding,
    AutoModelForMultipleChoice,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    DistilBertTokenizer,
    DistilBertForQuestionAnswering,
    AutoTokenizer,
    set_seed,
    DistilBertForMultipleChoice,
    DataCollatorForMultipleChoice,
)

set_seed(42)

tokenizer_path = "/content/drive/MyDrive/QA_Project_AML/roberta_model_optuna_best_params"
best_params_path = "/content/drive/MyDrive/QA_Project_AML/roberta_model_optuna_best_params/best_params.json"

with open(best_params_path, 'r') as f:
    best_params = json.load(f)
print(f"Best trial parameters saved to {best_params_path} are {best_params}.")

from datasets import Dataset

train_data = [json.loads(line) for line in open('/content/train_qafilt_true.json')]
val_data = [json.loads(line) for line in open('/content/val_qafilt_true.json')]
test_data = [json.loads(line) for line in open('/content/test_qafilt_true.json')]

train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)
test_dataset = Dataset.from_list(test_data)

recipeqa_dataset = DatasetDict({
    "train": train_dataset.shuffle(seed=42),
    "val": val_dataset.shuffle(seed=42),
    "test": test_dataset.shuffle(seed=42),
})

print(recipeqa_dataset)
recipeqa_dataset["train"][0]

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
max_length = 512

def preprocess_function(examples):
    all_input_ids = []
    all_attention_masks = []
    labels = []
    for i in range(len(examples["context"])):
        context_bodies = " ".join([step["body"] for step in examples["context"][i]])
        question_variants = [
            f"{examples['question_text'][i]} " +
            " ".join([q if q != "@placeholder" else choice for q in examples["question"][i]])
            for choice in examples["choice_list"][i]
        ]

        tokenized = tokenizer(
            [context_bodies] * len(question_variants),  
            question_variants,
            truncation="only_first",
            padding="max_length",
            max_length=max_length,
            return_tensors=None,
        )
        
        all_input_ids.append(tokenized["input_ids"])
        all_attention_masks.append(tokenized["attention_mask"])
        labels.append(examples["answer"][i])

    return {
        "input_ids": all_input_ids,
        "attention_mask": all_attention_masks,
        "labels": labels,
    }

print("Tokenization starts...")
tokenized_dataset = recipeqa_dataset.map(preprocess_function, batched=True, remove_columns=recipeqa_dataset["train"].column_names)
print("Tokenization finished.")

data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)

accuracy = evaluate.load("accuracy")
precision = evaluate.load("precision")
recall = evaluate.load("recall")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy_metric = accuracy.compute(predictions=predictions, references=labels)
    precision_score = precision.compute(predictions=predictions, references=labels, average="weighted")
    recall_score = recall.compute(predictions=predictions, references=labels, average="weighted")
    f1_score = f1.compute(predictions=predictions, references=labels, average="weighted")

    return {
        **accuracy_metric,
        'precision': precision_score['precision'],
        'recall': recall_score['recall'],
        'f1': f1_score['f1']
    }

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

print(f"Using device: {device} - {torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU'}")

final_roberta_model = AutoModelForMultipleChoice.from_pretrained("roberta-base")
final_roberta_model.config.attention_probs_dropout_prob = best_params["dropout_rate"]
final_roberta_model.config.hidden_dropout_prob = best_params["dropout_rate"]

final_training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/QA_Project_AML/roberta_model_optuna_final_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=best_params["num_train_epochs"],
    per_device_train_batch_size=best_params["batch_size"],
    per_device_eval_batch_size=best_params["batch_size"],
    learning_rate=best_params["learning_rate"],
    weight_decay=best_params["weight_decay"],
    adam_epsilon=best_params["adam_epsilon"],
    logging_steps=8,
    load_best_model_at_end=True,
    report_to="none",
    seed=42,
)

final_trainer = Trainer(
    model=final_roberta_model,
    args=final_training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["val"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

final_trainer.train()

final_model_path = "/content/drive/MyDrive/QA_Project_AML/roberta_recipeqa_final_best_model"
os.makedirs(final_model_path, exist_ok=True)

final_roberta_model.save_pretrained(final_model_path)
tokenizer.save_pretrained(final_model_path)
print(f"Final model and tokenizer saved to {final_model_path}")

final_results = final_trainer.evaluate(tokenized_dataset["val"])
print(f"Final Model Evaluation on Validation Set: {final_results}")

val_predictions = final_trainer.predict(tokenized_dataset["val"])
val_predicted_labels = np.argmax(val_predictions.predictions, axis=1)
val_true_labels = val_predictions.label_ids

print("\nSample Predictions:")
for idx in range(5):  
    example = recipeqa_dataset["val"][idx]
    full_question = f"{example['question_text']} " + " ".join([
        q if q != "@placeholder" else example["choice_list"][0]
        for q in example["question"]
    ])
    print(f"Q{idx + 1}: {full_question}")
    print("Choices:")
    for i, choice in enumerate(example["choice_list"]):
        selected = "✓" if i == val_predicted_labels[idx] else ""
        print(f"  {i}: {choice} {selected}")
    print(f"Predicted Answer: {example['choice_list'][val_predicted_labels[idx]]}")
    print(f"Actual Answer: {example['answer']}")

accuracy = accuracy_score(val_true_labels, val_predicted_labels)
f1 = f1_score(val_true_labels, val_predicted_labels, average='weighted')  

print(f"\n Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")

test_results = final_trainer.evaluate(tokenized_dataset['test'])
print(f"Final evaluation results on test set: {test_results}")

test_predictions = final_trainer.predict(tokenized_dataset["test"])
test_predicted_labels = np.argmax(test_predictions.predictions, axis=1)
test_true_labels = test_predictions.label_ids

print("\nSample Predictions on Test Set:")
for idx in range(5):  
    example = recipeqa_dataset["test"][idx]
    full_question = f"{example['question_text']} " + " ".join([
        q if q != "@placeholder" else example["choice_list"][0]
        for q in example["question"]
    ])
    print(f"Q{idx + 1}: {full_question}")
    print("Choices:")
    for i, choice in enumerate(example["choice_list"]):
        selected = "✓" if i == test_predicted_labels[idx] else ""
        print(f"  {i}: {choice} {selected}")
    print(f"Predicted Answer: {example['choice_list'][test_predicted_labels[idx]]}")
    print(f"Actual Answer: {example['answer']}")